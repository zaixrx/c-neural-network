// greek dictionary: Σ∂Δδ∇

#include <math.h>
#include <stdio.h>
#include <assert.h>
#include <stdlib.h>
#include <time.h>
#include "include/raylib.h"
#define STB_DS_IMPLEMENTATION
#include "include/stb_ds.h"

#define WIDTH (1600)
#define HEIGHT (900)

#define RADIUS (25)
#define HOR_COST (RADIUS * 2 + 250)
#define VER_COST (RADIUS * 2 + 50)
#define LINE_THICKNESS 3

int rnd() {
	srand(time(NULL));
	return rand();
}

typedef struct {
	size_t *layers;
	float **biases;
	float ***weights;
} Network;

Network *new_network(size_t *layers) {
	Network *net = malloc(sizeof(Network));
	net->layers = layers;
	assert(arrlen(net->layers) >= 2);
	for (int l = 0; l < arrlen(layers); ++l) {
		float *biases = NULL;
		for (int i = 0; i < layers[l]; ++i) {
			arrpush(biases, 69); // TODO: must be random
		}
		arrpush(net->biases, biases);
	}
	for (int l = 1; l < arrlen(layers); ++l) {
		float **matrix = NULL;
		for (int j = 0; j < layers[l]; ++j) {
			float *column = NULL;
			for (int i = 0; i < layers[l-1]; ++i) {
				arrpush(column, 69);
			}
			arrpush(matrix, column);
		}
		arrpush(net->weights, matrix);
	}
	return net;
}

void
DrawNeurons(Network *net) {
	static Color rgb[] = { RED, GREEN, BLUE };

	clock_t start = clock();
	float x = (WIDTH - (arrlen(net->layers) - 1) * HOR_COST) / 2.0f;
	DrawCircle(x, HEIGHT >> 1, RADIUS, RED);
	for (int l = 0; l < arrlen(net->layers); ++l) {
		float y = (HEIGHT - (net->layers[l] - 1) * VER_COST) / 2.0f;
		printf("%f\n", x);
		for (int n = 0; n < net->layers[l]; ++n) {
			DrawCircle(x, y, RADIUS, WHITE);
			if (l + 1 < arrlen(net->layers)) {
				float ny = (HEIGHT - (net->layers[l+1]-1) * VER_COST) / 2.0f;
				for (int nn = 0; nn < net->layers[l+1]; ++nn) {
					DrawLineEx(
						(Vector2){x + RADIUS, y},
						(Vector2){x + HOR_COST - RADIUS, ny},
						LINE_THICKNESS,
						rgb[l % 3]
					);
					ny += VER_COST;
				}
			}
			y += VER_COST;
		}
		x += HOR_COST;
	}
	DrawCircle(x, HEIGHT >> 1, RADIUS, RED);
	clock_t end = clock();

	printf("DrawNeurons :: took %fµs\n", (float)(end - start));
}

inline float
sigmoid_prime(float x) {
	float expo = exp(-x);
	return expo / ((1 + expo) * (1 + expo));
}

inline float
sigmoid(float x) {
	return 1 / (1 + exp(-x));
}

inline float
*v_sigmoid(float *x) {
	float *a = NULL;
	arrsetlen(a, arrlen(x));
	for (int i = 0; i < arrlen(x); ++i) {
		a[i] = sigmoid(x[i]);
	}
	return a;
}

float
*feed_forward(Network *net, float *data) {
	assert(arrlen(data) == net->layers[0] && "input's length must match input layer's");
	for (int l = 1; l < arrlen(net->layers); ++l) {
		float *output = NULL;
		for (int nn = 0; nn < net->layers[l]; ++nn) {
			float sum = net->biases[l][nn];
			for (int n = 0; n < net->layers[l-1]; ++n) {
				sum += data[n] * net->weights[l][nn][n];
			}
			arrpush(output, sigmoid(sum));
		}
		arrfree(data);
		data = output;
	}
	return data;
}

void
shuffle_array(float *arr) {
	for (int i = 0; i < arrlen(arr); ++i) {
		int j = rnd() % arrlen(arr);
		arr[i] += arr[j];
		arr[j] -= arr[i];
		arr[i] -= arr[j];
	}
}

float
**generate_mini_batches(float *data, size_t mini_batch_size) {
	float **mini_batches = NULL;
	for (int i = 0; i < arrlen(data); i += mini_batch_size) {
		float *mini_batch = NULL;
		arrsetlen(mini_batch, mini_batch_size);
		memcpy(mini_batch, data + i, mini_batch_size);
		arrpush(mini_batches, mini_batch);
	}
	return NULL;
}

void
backprop(Network *net, float *x, float *y, float ***nabla_w_x, float **nabla_b_x) {
/*
foreach test input we want to calculate the change of each weight and bias:
	∇C = (∂C/∂W, ∂C/∂B)

for l > 1: δl = ∂C/∂Z(l)
1) δL = hadamard(y - aL, sigma_prime(zL))
2) for l < L: δ(l) = hadamard(dot(W(l+1), δ(l+1)), sigma_prime(Z(l)))

-- finally --
given Z(l) = dot(W(l), A(l-1)) + B(l) we conclude:
     	∂C/∂W(l) = hadamard(δ(l), A(l-1)) ... (1)
also:
     	∂C/∂B(l) = δ(l) ... (2)
hence:
   	∇C(l) = (∂C/∂W(l), ∂C/∂B(l))
do that foreach l > 1 and you get ∇C for a specific test input.
*/
	size_t L = arrlen(net->layers);
        float **Z = NULL; // @heap_allocated list to store all the z vectors, layer by layer
        float **A= NULL; // @heap_allocated
        float *a = x;
	arrpush(A, a);
	for (int l = 0; l < L - 1; ++l) {
		float **w_l = net->weights[l];
		float *b_l = net->biases[l];
		float *z = NULL; // @heap_allocated
		// calculate weighted sum: matrix_vector_mul, vector_add
		for (int nn = 0; nn < net->layers[l+1]; ++nn) {
			float sum = b_l[nn];
			for (int n = 0; n < net->layers[l]; ++n) {
				sum += a[n] * w_l[nn][n];
			}
			arrpush(z, sum);
		}
		arrpush(Z, z);
		a = v_sigmoid(z);
		arrpush(A, a);
	}

	// δL = hadamard(y - aL, sigma_prime(zL))
	float *delta_L = NULL; // @heap_allocated
	arrsetlen(delta_L, arrlen(y));
	for (int o = 0; o < arrlen(y); ++o) {
		delta_L[o] = (y[o] - A[L-1][o]) * sigmoid_prime(Z[L-1][o]);
	}
	nabla_b_x[L-1] = delta_L;

	arrsetlen(nabla_b_x, L); // @heap_allocated
	arrsetlen(nabla_w_x, L); // @heap_allocated

	// for l < L: δ(l) = hadamard(dot(W(l+1), δ(l+1)), sigma_prime(Z(l)))
	float *delta = delta_L;
	for (int l = L-2; l > 1; --l) {
		float *new_delta = NULL;
		// dot(W(l+1), δ(l+1)
		for (int j = 0; j < net->layers[l+1]; ++j) {
			float sum = 0.0f;
			for (int k = 0; k < net->layers[l]; ++k) {
				sum += delta[k] * net->weights[l+1][j][k];
			}
			arrpush(new_delta, sum);
		}
		// hadamard(new_data, sigma_prime(Z(l)))
		for (int o = 0; o < arrlen(y); ++o) {
			new_delta[o] *= sigmoid_prime(Z[l][o]);
		}
		// ∂C/∂W(l)(jk) = δ(l)(j) * A(l-1)(k)
		float *matrix = NULL;
		arrsetlen(matrix, net->weights[l]);
		for (int j = 0; j < arrlen(net->weights[l]); ++j) {
			float *vector = NULL;
			for (int k = 0; k < arrlen(net->weights[l][j]); ++k) {
				arrpush(vector, new_delta[j] * A[l-1][j]);
			}
		}
		// ∂C/∂B(l) = δL
		nabla_b_x[l] = delta = new_delta;
	}
}

// mini_batch = [
// 	batch: [x, y, x', y' ...],
// 	batch: [x, y, x', y' ...],
// ]
void
update_mini_batch(Network *net, float **mini_batch, float eta) {
	float **nabla_b = NULL;
	float ***nabla_w = NULL;
	assert(arrlen(mini_batch) % 2 == 0);
	for (int b = 0; b < arrlen(mini_batch); b += 2) {
		backprop(net, x, y, nabla_w, nabla_b);
	}
	for (int bl = 0; bl < arrlen(net->biases); ++bl) {
		for (int b = 0; b < arrlen(net->biases[bl]); ++b) {
			net->biases[bl][b] += -eta * 0.5f / arrlen(mini_batch) * nabla_b[bl][b];
		}
	}
	for (int wl= 0; wl < arrlen(net->weights); ++wl) {
		for (int w = 0; w < arrlen(net->weights[wl]); ++w) {
			for (int pw = 0; pw < arrlen(net->weights[pw][w]); ++pw) {
				net->weights[wl][w][pw] += -eta * 0.5f / arrlen(mini_batch) * nabla_w[wl][w][pw];
			}
		}
	}
}

void
SGD(Network *net, float *training_data, size_t epochs, size_t mini_batch_size, float eta) {
	assert(arrlen(training_data) == net->layers[0] && "training_data's length must match input layer's");
	for (int e = 0; e < epochs; ++e) {
		shuffle_array(training_data);
		float **mini_batches = generate_mini_batches(training_data, mini_batch_size);
		for (int b = 0; b < arrlen(mini_batches); ++b) {
			update_mini_batch(net, mini_batches[b], eta);
			arrfree(mini_batches[b]);
		}
		arrfree(mini_batches);
	}
}

int
main(void) {
	Network *net = NULL;
	{
		size_t *layers = NULL;
		arrpush(layers, 5);
		arrpush(layers, 7);
		arrpush(layers, 3);
		arrpush(layers, 8);
		arrpush(layers, 1);
		net = new_network(layers);
	}

	InitWindow(WIDTH, HEIGHT, "Hello World!");
	SetTargetFPS(1);

    	while (!WindowShouldClose()) {
    	    	BeginDrawing();
			ClearBackground(BLACK);
			DrawNeurons(net);
    	    	EndDrawing();
    	}

    	CloseWindow();

    	return 0;
}
